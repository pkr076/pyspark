{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.3.0-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext,SparkConf\n",
    "conf = SparkConf().setAppName(\"word count\").setMaster(\"local[3]\")\n",
    "sc =SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[3]\",\"word count\")\n",
    "data = sc.parallelize([8,4,6,2])\n",
    "data.map(lambda x:x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The : 10\n",
      "history : 1\n",
      "of : 33\n",
      "New : 20\n",
      "York : 17\n",
      "begins : 1\n",
      "around : 4\n",
      "10,000 : 1\n",
      "BC, : 1\n",
      "when : 1\n",
      "the : 71\n",
      "first : 5\n",
      "Native : 2\n",
      "Americans : 1\n",
      "arrived. : 1\n",
      "By : 1\n",
      "1100 : 1\n",
      "AD, : 1\n",
      "York's : 2\n",
      "main : 2\n",
      "native : 1\n",
      "cultures, : 1\n",
      "Iroquoian : 1\n",
      "and : 21\n",
      "Algonquian, : 1\n",
      "had : 1\n",
      "developed. : 1\n",
      "European : 2\n",
      "discovery : 1\n",
      "was : 8\n",
      "led : 2\n",
      "by : 2\n",
      "French : 1\n",
      "in : 21\n",
      "1524 : 1\n",
      "land : 1\n",
      "claim : 1\n",
      "came : 2\n",
      "1609 : 1\n",
      "Dutch. : 1\n",
      "As : 1\n",
      "part : 1\n",
      "Netherland, : 1\n",
      "colony : 2\n",
      "important : 1\n",
      "fur : 1\n",
      "trade : 3\n",
      "eventually : 1\n",
      "became : 6\n",
      "an : 1\n",
      "agricultural : 1\n",
      "resource : 1\n",
      "thanks : 1\n",
      "to : 17\n",
      "patroon : 1\n",
      "system. : 1\n",
      "In : 3\n",
      "1626 : 1\n",
      "Dutch : 1\n",
      "bought : 1\n",
      "island : 1\n",
      "Manhattan : 1\n",
      "from : 8\n",
      "Americans.[1] : 1\n",
      "1664, : 1\n",
      "England : 1\n",
      "renamed : 1\n",
      "York, : 1\n",
      "after : 1\n",
      "Duke : 1\n",
      "(later : 1\n",
      "James : 1\n",
      "II : 2\n",
      "& : 1\n",
      "VII.) : 1\n",
      "City : 7\n",
      "gained : 1\n",
      "prominence : 1\n",
      "18th : 1\n",
      "century : 1\n",
      "as : 4\n",
      "a : 11\n",
      "major : 3\n",
      "trading : 1\n",
      "port : 2\n",
      "Thirteen : 2\n",
      "Colonies. : 1\n",
      " : 7\n",
      "played : 1\n",
      "pivotal : 1\n",
      "role : 1\n",
      "during : 3\n",
      "American : 3\n",
      "Revolution : 1\n",
      "subsequent : 1\n",
      "war. : 2\n",
      "Stamp : 1\n",
      "Act : 1\n",
      "Congress : 1\n",
      "1765 : 1\n",
      "brought : 1\n",
      "together : 1\n",
      "representatives : 1\n",
      "across : 1\n",
      "Colonies : 1\n",
      "form : 1\n",
      "unified : 1\n",
      "response : 1\n",
      "British : 3\n",
      "policies. : 1\n",
      "Sons : 1\n",
      "Liberty : 2\n",
      "were : 3\n",
      "active : 1\n",
      "challenge : 1\n",
      "authority. : 1\n",
      "After : 1\n",
      "loss : 1\n",
      "at : 2\n",
      "Battle : 2\n",
      "Long : 1\n",
      "Island, : 1\n",
      "Continental : 1\n",
      "Army : 1\n",
      "suffered : 1\n",
      "series : 1\n",
      "additional : 1\n",
      "defeats : 1\n",
      "that : 1\n",
      "forced : 1\n",
      "retreat : 1\n",
      "area, : 1\n",
      "leaving : 1\n",
      "strategic : 1\n",
      "harbor : 1\n",
      "army : 1\n",
      "navy : 1\n",
      "their : 2\n",
      "North : 1\n",
      "base : 2\n",
      "operations : 1\n",
      "for : 3\n",
      "rest : 1\n",
      "Saratoga : 1\n",
      "turning : 1\n",
      "point : 2\n",
      "war : 1\n",
      "favor : 1\n",
      "Americans, : 1\n",
      "convincing : 1\n",
      "France : 1\n",
      "formally : 1\n",
      "ally : 1\n",
      "with : 3\n",
      "them. : 1\n",
      "constitution : 1\n",
      "adopted : 1\n",
      "1777, : 1\n",
      "strongly : 1\n",
      "influenced : 1\n",
      "United : 4\n",
      "States : 3\n",
      "Constitution. : 2\n",
      "national : 2\n",
      "capital : 3\n",
      "various : 1\n",
      "times : 1\n",
      "between : 1\n",
      "1785 : 1\n",
      "1790, : 1\n",
      "where : 1\n",
      "Bill : 1\n",
      "Rights : 1\n",
      "drafted. : 1\n",
      "Albany : 1\n",
      "permanent : 1\n",
      "state : 5\n",
      "1797. : 1\n",
      "1787, : 1\n",
      "eleventh : 1\n",
      "ratify : 1\n",
      "hosted : 2\n",
      "significant : 2\n",
      "transportation : 2\n",
      "advancements : 2\n",
      "19th : 2\n",
      "century, : 3\n",
      "including : 1\n",
      "steamboat : 1\n",
      "line : 1\n",
      "1807, : 1\n",
      "Erie : 1\n",
      "Canal : 1\n",
      "1825, : 1\n",
      "America's : 1\n",
      "regularly : 1\n",
      "scheduled : 1\n",
      "rail : 1\n",
      "service : 2\n",
      "1831. : 1\n",
      "These : 1\n",
      "expanded : 1\n",
      "settlement : 1\n",
      "western : 1\n",
      "ties : 2\n",
      "Midwest : 1\n",
      "settlements : 1\n",
      "Great : 3\n",
      "Lakes. : 1\n",
      "Due : 1\n",
      "City's : 1\n",
      "South, : 1\n",
      "there : 1\n",
      "numerous : 1\n",
      "southern : 2\n",
      "sympathizers : 1\n",
      "early : 1\n",
      "days : 1\n",
      "Civil : 1\n",
      "War : 2\n",
      "mayor : 1\n",
      "proposed : 1\n",
      "secession. : 1\n",
      "Far : 1\n",
      "any : 1\n",
      "battles, : 1\n",
      "ultimately : 1\n",
      "sent : 1\n",
      "most : 3\n",
      "men : 1\n",
      "money : 1\n",
      "support : 1\n",
      "Union : 1\n",
      "cause. : 1\n",
      "Thereafter, : 1\n",
      "helped : 1\n",
      "create : 1\n",
      "industrial : 1\n",
      "age : 1\n",
      "consequently : 1\n",
      "home : 1\n",
      "some : 1\n",
      "labor : 1\n",
      "unions. : 1\n",
      "During : 1\n",
      "entry : 1\n",
      "immigrants : 1\n",
      "States, : 1\n",
      "beginning : 1\n",
      "wave : 1\n",
      "Irish : 1\n",
      "Famine. : 1\n",
      "Millions : 1\n",
      "through : 1\n",
      "Castle : 1\n",
      "Clinton : 1\n",
      "Battery : 1\n",
      "Park : 1\n",
      "before : 2\n",
      "Ellis : 1\n",
      "Island : 1\n",
      "opened : 3\n",
      "1892 : 1\n",
      "welcome : 1\n",
      "millions : 1\n",
      "more, : 1\n",
      "increasingly : 1\n",
      "eastern : 1\n",
      "Europe. : 1\n",
      "Statue : 1\n",
      "1886 : 1\n",
      "symbol : 1\n",
      "hope. : 1\n",
      "boomed : 1\n",
      "Roaring : 1\n",
      "Twenties, : 1\n",
      "Wall : 1\n",
      "Street : 1\n",
      "Crash : 1\n",
      "1929, : 1\n",
      "skyscrapers : 1\n",
      "expressed : 1\n",
      "energy : 1\n",
      "city. : 1\n",
      "site : 1\n",
      "successive : 1\n",
      "tallest : 1\n",
      "buildings : 1\n",
      "world : 1\n",
      "1913–74. : 1\n",
      "buildup : 1\n",
      "defense : 1\n",
      "industries : 1\n",
      "World : 2\n",
      "turned : 1\n",
      "state's : 2\n",
      "economy : 1\n",
      "Depression, : 1\n",
      "hundreds : 1\n",
      "thousands : 1\n",
      "worked : 1\n",
      "defeat : 1\n",
      "Axis : 1\n",
      "powers. : 1\n",
      "Following : 2\n",
      "war, : 1\n",
      "experienced : 1\n",
      "suburbanization : 1\n",
      "all : 1\n",
      "cities, : 1\n",
      "central : 1\n",
      "cities : 1\n",
      "shrank. : 1\n",
      "Thruway : 1\n",
      "system : 1\n",
      "1956, : 1\n",
      "signalling : 1\n",
      "another : 1\n",
      "era : 1\n",
      "advances. : 1\n",
      "period : 1\n",
      "near–bankruptcy : 1\n",
      "late : 1\n",
      "1970s, : 1\n",
      "renewed : 1\n",
      "its : 2\n",
      "stature : 1\n",
      "cultural : 1\n",
      "center, : 1\n",
      "attracted : 1\n",
      "more : 1\n",
      "immigration, : 1\n",
      "development : 1\n",
      "new : 1\n",
      "music : 1\n",
      "styles. : 1\n",
      "city : 1\n",
      "developed : 1\n",
      "publishing : 1\n",
      "become : 1\n",
      "media : 1\n",
      "over : 1\n",
      "second : 1\n",
      "half : 1\n",
      "20th : 1\n",
      "hosting : 1\n",
      "news : 1\n",
      "channels : 1\n",
      "broadcasts. : 1\n",
      "Some : 1\n",
      "newspapers : 1\n",
      "nationally : 1\n",
      "globallyrenowned. : 1\n",
      "manufacturing : 1\n",
      "eroded : 1\n",
      "restructuring : 1\n",
      "industry, : 1\n",
      "transitioned : 1\n",
      "into : 1\n",
      "industries. : 1\n",
      "September : 1\n",
      "11 : 1\n",
      "attacks : 2\n",
      "2001 : 1\n",
      "destroyed : 1\n",
      "Trade : 1\n",
      "Center, : 1\n",
      "killing : 1\n",
      "almost : 1\n",
      "3,000 : 1\n",
      "people; : 1\n",
      "they : 1\n",
      "largest : 1\n",
      "terrorist : 1\n",
      "on : 1\n",
      "soil. : 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lines = sc.textFile(\"word_count.text\")\n",
    "words = lines.flatMap(lambda line:line.split(' '))\n",
    "wordCount = words.countByValue()\n",
    "for word, count in wordCount.items():\n",
    "    print('{} : {}'.format(word,count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o146.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0cfa29b095c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\tmp\\hive\\wordCount1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   2203\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2204\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2205\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2207\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o146.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n"
     ]
    }
   ],
   "source": [
    "words.saveAsTextFile(\"C:\\\\tmp\\hive\\wordCount1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
